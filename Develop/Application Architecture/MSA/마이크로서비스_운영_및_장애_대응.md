---
title: 마이크로서비스 로그 운영 및 장애 대응 (ClickHouse / AWS Grafana / CloudWatch / Elasticsearch)
tags: [logging, clickhouse, cloudwatch, grafana, elasticsearch, observability]
updated: 2025-12-19
---

# 마이크로서비스 로그 운영 및 장애 대응

## 개요

마이크로서비스 환경에서 장애 대응 속도는 로그 구조에 의해 결정된다.  
메트릭과 트레이스도 중요하지만, 실제 장애 원인은 대부분 로그에서 시작한다.

이 문서는 실제 운영 환경에서 ClickHouse, CloudWatch, Grafana, Elasticsearch를 사용해 로그를 수집·조회·분석했던 경험을 기준으로 정리한다.  
툴 설명보다 **어떻게 써야 문제를 빨리 찾는지**에 초점을 둔다.

---

## 로그 설계 기본 원칙

### 로그는 반드시 구조화한다

문자열 로그는 초기에 편하지만, 운영 단계에서 바로 한계를 맞는다.

```json
{
  "timestamp": "2025-12-16T11:32:21.123Z",
  "service": "order-service",
  "level": "ERROR",
  "message": "payment timeout",
  "orderId": "ORD-20251216-9912",
  "userId": "U-10291",
  "latencyMs": 3200,
  "traceId": "abc123"
}
```

- grep 대신 필드 기반 검색 가능
- 특정 주문, 사용자 단위 추적 가능
- ClickHouse, Elasticsearch 모두 대응 가능

운영 중 문자열 로그만 남겨둔 서비스는 결국 다시 손댄다.

---

## CloudWatch 로그 활용 (AWS 기본 스택)

### 언제 CloudWatch를 쓰는가

- ECS / EKS 초기 단계
- 로그 볼륨이 아직 크지 않은 경우
- 운영 인력 최소화 상태

### 로그 그룹 구조

```
/ecs/order-service
/ecs/payment-service
/ecs/auth-service
```

서비스 단위로 반드시 분리한다.  
환경(dev, stage, prod)은 로그 그룹 이름에 포함시킨다.

### CloudWatch Logs Insights 실사용 쿼리

#### 특정 주문 장애 추적

```
fields @timestamp, message, orderId
| filter orderId = "ORD-20251216-9912"
| sort @timestamp asc
```

#### 타임아웃 로그만 추출

```
fields @timestamp, service, latencyMs
| filter message like /timeout/
| filter latencyMs > 3000
```

Insights를 쓰지 않고 콘솔 검색만 쓰는 경우, 장애 분석 시간이 몇 배로 늘어난다.

### 한계

- 대량 로그 조회 시 비용 증가
- 복잡한 집계 쿼리는 부적합
- 장기 보관에는 부적절

이 시점부터 ClickHouse 또는 Elasticsearch가 필요해진다.

---

## ClickHouse 로그 분석

### ClickHouse를 선택한 이유

- 초당 수십만 로그 수집 가능
- 비용 대비 성능이 매우 좋다
- 장애 시점 분석에 압도적으로 빠르다

로그를 OLAP 데이터로 본다.

### 로그 테이블 예시

```sql
CREATE TABLE logs (
  timestamp DateTime,
  service String,
  level String,
  message String,
  orderId String,
  userId String,
  latencyMs UInt32,
  traceId String
)
ENGINE = MergeTree
ORDER BY (service, timestamp);
```

### 장애 분석 실전 쿼리

#### 특정 시간대 에러 증가 확인

```sql
SELECT
  toStartOfMinute(timestamp) AS minute,
  count() AS error_count
FROM logs
WHERE level = 'ERROR'
  AND timestamp >= now() - INTERVAL 30 MINUTE
GROUP BY minute
ORDER BY minute;
```

#### 특정 서비스 응답 지연 원인 추적

```sql
SELECT
  message,
  count()
FROM logs
WHERE service = 'payment-service'
  AND latencyMs > 3000
GROUP BY message
ORDER BY count() DESC;
```

이 쿼리 하나로 장애 원인 로그 패턴이 바로 드러난다.

### 운영 시 주의사항

- ORDER BY 키를 잘못 잡으면 성능이 급격히 떨어진다
- TTL 설정으로 오래된 로그 자동 삭제 필요
- raw 로그와 집계용 로그 테이블을 분리한다

---

## Elasticsearch 로그 활용

### 언제 Elasticsearch를 쓰는가

- 로그 검색 UI가 중요한 경우
- 운영팀, CS팀도 로그를 직접 보는 환경
- 텍스트 검색 비중이 높은 경우

### 인덱스 전략

```
logs-order-service-2025.12.16
logs-payment-service-2025.12.16
```

서비스 + 날짜 조합을 기본으로 한다.

### Kibana 실전 검색 패턴

#### 특정 에러 메시지 빈도

```
message: "timeout" AND service: "payment-service"
```

#### 사용자 단위 로그 추적

```
userId: "U-10291"
```

### 한계

- 로그 볼륨이 커질수록 비용 증가
- 장기 보관 비용 부담
- 대규모 집계는 ClickHouse보다 느림

그래서 실무에서는 **Elasticsearch + ClickHouse 병행** 구조를 쓴다.

---

## Grafana 로그 활용 (AWS Grafana 포함)

### Grafana를 로그에 쓰는 이유

Grafana는 메트릭만 보는 도구가 아니다.  
로그 + 메트릭을 한 화면에서 본다.

### Grafana + CloudWatch

- 장애 발생 시 메트릭 그래프 클릭
- 해당 시점의 로그 자동 필터링
- 장애 시점과 로그를 동시에 본다

### Grafana + ClickHouse

```sql
SELECT
  timestamp,
  service,
  message
FROM logs
WHERE timestamp BETWEEN $__from AND $__to
  AND level = 'ERROR'
```

Grafana Time Range와 쿼리를 직접 연결한다.

### 운영에서 가장 자주 쓰는 패턴

- 에러율 그래프 클릭 → 로그 패널 자동 필터
- 특정 서비스 선택 → 해당 서비스 로그만 표시
- 주문 ID 입력 → 전체 서비스 로그 연쇄 조회

---

## 로그 기반 장애 대응 흐름

실제 운영에서 쓰는 순서다.

1. Grafana에서 에러율 상승 감지
2. 해당 시간대 로그 필터
3. orderId / traceId 기준으로 흐름 확인
4. ClickHouse로 패턴 집계
5. 재현 가능한 원인 코드 확인

로그 구조가 나쁘면 이 과정이 불가능하다.

---

## 실무에서 겪은 문제와 해결

### 문제 1. 로그는 많은데 원인이 안 보임

원인:
- message 문자열 위주 로그
- context 정보 없음

해결:
- 반드시 business key(orderId 등) 포함
- latency, resultCode 같은 수치 필드 추가

### 문제 2. 장애 후 로그 조회가 너무 느림

원인:
- Elasticsearch 단독 사용
- 인덱스 설계 미흡

해결:
- ClickHouse 병행
- 장애 분석은 ClickHouse, 검색은 ES

### 문제 3. 로그 비용 폭증

원인:
- DEBUG 로그 상시 활성화
- 성공 로그 무차별 수집

해결:
- ERROR/WARN 100%
- INFO 선택적
- 성공 로그 샘플링

---

## 정리

- CloudWatch는 시작점이다
- 운영 단계에서는 ClickHouse가 핵심이다
- Elasticsearch는 검색 UX 용도다
- Grafana는 로그와 메트릭을 연결하는 도구다
- 로그 설계가 곧 장애 대응 속도다

문제는 로그가 부족해서가 아니라, **쓸 수 없어서** 발생한다.
